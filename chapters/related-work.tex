\section{Related Work}

\textbf{Deep Supervised Hashing (DSH)}

\textit{Approach}

The approach begins by designing a CNN model that processes image pairs alongside labels indicating whether the two images are similar or not, and outputs binary codes. To increase efficiency, image pairs are generated dynamically during training, enabling the utilization of a significantly larger number of pairs. The loss function is crafted to bring the outputs of similar image pairs closer together while pushing the outputs of dissimilar pairs further apart. This ensures that the resulting Hamming space effectively reflects the semantic structure of the images. Since optimizing directly in Hamming space is nondifferentiable, the network outputs are relaxed to continuous values during training. At the same time, a regularization term is introduced to drive the continuous outputs toward discrete binary values. Using this framework, images are encoded by passing them through the network, followed by converting the network's outputs into binary code representations via quantization.

\textit{Image retrieval process within DSH}

During the training phase, the DSH model learns to map images into binary hash codes due to the CNN. The network is trained with pairs of images labeled as similar or dissimilar, ensuring that semantically similar images are mapped to similar binary codes. And when a query image is provided, it is passed through the CNN to generate its binary hash code. That code shall then be compared with other binary hash codes within the dataset(s). Calculating Hamming distance will provide the number of differing bits between the codes. After all the calculations, the image with hash codes that have the smallest Hamming distances to the query's hash code are retrieved as results.

\textit{Training Methodology}

Traditional Deep Supervised Hashing (DSH) methods often utilize a Siamese network structure for generating image pairs offline. However, this approach is constrained by storage limitations, as a dataset of $n$ images results in storing $n/2$ valid pairs. To improve scalability, modern methods generate image pairs dynamically within mini-batches, ensuring diverse pairs across iterations without requiring pre-stored similarity matrices. This enhances computational efficiency, making it viable for large-scale datasets. Additionally, reusing early network layers across different hash code lengths optimizes memory usage, but increasing code length adds model complexity, necessitating careful regularization to prevent overfitting.\cite{Liu_2016_CVPR}

\textbf{Feature Learning-Based Deep Supervised Hashing with Pairwise Labels}

\textit{Training Methodology}

DPSH employs pairwise similarity labels to learn hash codes that align closely for semantically related images. The training set is composed of a small labeled subset (seed set), a large unlabeled pool, and a query set whose labels are dynamically acquired. Active learning is employed iteratively: (1) training on the labeled set, (2) selecting the most informative samples from the unlabeled pool, and (3) acquiring labels from an oracle. Hash codes are generated by projecting images through the network, where binarization is performed using thresholding. To optimize retrieval efficiency, a loss function enforces compactness for similar pairs and divergence for dissimilar ones. Due to the inherent difficulty in direct binary optimization, the model relaxes constraints using continuous approximations while balancing objectives like pairwise similarity and quantization error. Semi-supervised learning strategies further enhance performance by incorporating both labeled and unlabeled data.\cite{li2016featurelearningbaseddeep}

\textbf{Deep Supervised Discrete Hashing (DSDH or DDSH)}

\textit{Core Contributions and Training Strategy}

DSDH explicitly optimizes the final layer of the model to output discrete binary hash codes, ensuring consistency with pairwise labels and class information. This marks one of the first attempts to integrate classification and pairwise similarity learning into a unified hashing framework. Quantization errors, a common challenge in hashing, are mitigated through an alternating minimization approach, employing discrete cyclic coordinate descent for efficient optimization.

The binary hash learning process iteratively updates hash bits while ensuring similarity preservation. The $k$-th bit of each data point is optimized by minimizing an objective function that enforces binary constraints ($-1$ or $+1$ values). To address the non-differentiability of the sign function, activation functions like hyperbolic tangent provide smooth approximations, enabling gradient-based updates. The training alternates between optimizing hash codes and refining neural network parameters via backpropagation. Experiments demonstrate that DSDH surpasses prior approaches in retrieval efficiency, particularly in balancing discrete optimization with deep learning.\cite{10.5555/3294996.3295009}

However, existing DSH models still face three key challenges: (1) semantic preservation—many methods struggle to retain fine-grained relationships between images; (2) scalability—computational costs remain high for large-scale datasets; and (3) robustness—hashing methods are sensitive to variations in image quality and distribution.
    % Traditional Hashing Methods: Locality-Sensitive Hashing (LSH): A pioneering method for approximate nearest neighbor search, though it struggles with semantic preservation in complex datasets.


    % \begin{itemize}
    %     \item Deep Supervised Hashing (DSH): Leverages labeled data to train deep networks for generating semantic-aware hash codes. 
    %     % https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_Deep_Supervised_Hashing_CVPR_2016_paper.pdf?form=MG0AV3
        
    %     \item Feature Learning based Deep Supervised Hashing with Pairwise Labels: Performs simultaneous feature learning and hash-code learning for applications with pairwise labels. 
    %     % https://arxiv.org/abs/1511.03855

    %     \item Deep supervised discrete hashing: Outputs of the last layer are constrained to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function.
    %     % https://dl.acm.org/doi/10.5555/3294996.3295009
    % \end{itemize}
         


% We first introduce the problem setting of deep hashing with active pairwise supervision and then build the link between the acquisition function for active annotation and the structural risk minimization principle. Finally, we design the acquisition function by considering pairwise relationships for active deep hashing. 