%===============================================================================
% Sample bibliopgraphy file for ifaconf.bst style to be used in
% IFAC meeting papers
% Copyright (c) 2007-2008 International Federation of Automatic Control
%===============================================================================
@InProceedings{Liu_2016_CVPR,
  author = {Liu, Haomiao and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
  title = {Deep Supervised Hashing for Fast Image Retrieval},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month = {June},
  year = {2016}
}


@misc{li_2016,
      title={Feature Learning based Deep Supervised Hashing with Pairwise Labels}, 
      author={Wu-Jun Li and Sheng Wang and Wang-Cheng Kang},
      year={2016},
      eprint={1511.03855},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.03855}, 
}

@inproceedings{dsdh_2017,
author = {Li, Qi and Sun, Zhenan and He, Ran and Tan, Tieniu},
title = {Deep supervised discrete hashing},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {2479â€“2488},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@article{approx_nearest,
author = {Andoni, Alexandr and Indyk, Piotr},
title = {Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions},
year = {2008},
issue_date = {January 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/1327452.1327494},
doi = {10.1145/1327452.1327494},
abstract = {In this article, we give an overview of efficient algorithms for the approximate and exact nearest neighbor problem. The goal is to preprocess a dataset of objects (e.g., images) so that later, given a new query object, one can quickly return the dataset object that is most similar to the query. The problem is of significant interest in a wide variety of areas.},
journal = {Commun. ACM},
month = jan,
pages = {117-122},
numpages = {6}
}

@inproceedings{890d94598fa54dac85eae9aa824a0a7e,
title = "Spectral hashing",
abstract = "Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of finding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigen- vectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to efficiently calculate the code of a novel data- point. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outper- form the state-of-the art.",
author = "Yair Weiss and Antonio Torralba and Rob Fergus",
year = "2009",
language = "English (US)",
isbn = "9781605609492",
series = "Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference",
publisher = "Neural Information Processing Systems",
pages = "1753--1760",
booktitle = "Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference",
note = "22nd Annual Conference on Neural Information Processing Systems, NIPS 2008 ; Conference date: 08-12-2008 Through 11-12-2008",
}

@article{6296665,
  author={Gong, Yunchao and Lazebnik, Svetlana and Gordo, Albert and Perronnin, Florent},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Iterative Quantization: A Procrustean Approach to Learning Binary Codes for Large-Scale Image Retrieval}, 
  year={2013},
  volume={35},
  number={12},
  pages={2916-2929},
  keywords={Quantization;Binary codes;Principal component analysis;Encoding;Linear programming;Iterative methods;Large-scale image search;binary codes;hashing;quantization},
  doi={10.1109/TPAMI.2012.193}}

@misc{cao2017hashnetdeeplearninghash,
      title={HashNet: Deep Learning to Hash by Continuation}, 
      author={Zhangjie Cao and Mingsheng Long and Jianmin Wang and Philip S. Yu},
      year={2017},
      eprint={1702.00758},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1702.00758}, 
}

@misc{gordo2017endtoendlearningdeepvisual,
      title={End-to-end Learning of Deep Visual Representations for Image Retrieval}, 
      author={Albert Gordo and Jon Almazan and Jerome Revaud and Diane Larlus},
      year={2017},
      eprint={1610.07940},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1610.07940}, 
}